{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8da4bd-da9e-4991-949c-6b65891edcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 STATISTICAL ANALYSIS HELPER\n",
      "==================================================\n",
      "📊 DATA LOADED\n",
      "========================================\n",
      "True Count (Deterministic): 290\n",
      "ChatGPT_Scan: 78\n",
      "Claude_Addition: 91\n",
      "Claude_Fabrication: 115\n",
      "\n",
      "🔬 NULL HYPOTHESES FORMULATED\n",
      "==================================================\n",
      "\n",
      "ACCURACY_NULL:\n",
      "  H₀: AI systems can accurately count P0 failures in documents (μ_error = 0)\n",
      "\n",
      "ACCURACY_ALTERNATIVE:\n",
      "  H₁: AI systems cannot accurately count P0 failures in documents (μ_error ≠ 0)\n",
      "\n",
      "CONSISTENCY_NULL:\n",
      "  H₀: AI systems produce consistent counts across different approaches (σ² = 0)\n",
      "\n",
      "CONSISTENCY_ALTERNATIVE:\n",
      "  H₁: AI systems produce inconsistent counts across different approaches (σ² > 0)\n",
      "\n",
      "EQUIVALENCE_NULL:\n",
      "  H₀: AI counting performance is equivalent to deterministic methods (|μ_AI - μ_true| ≤ δ)\n",
      "\n",
      "EQUIVALENCE_ALTERNATIVE:\n",
      "  H₁: AI counting performance is not equivalent to deterministic methods (|μ_AI - μ_true| > δ)\n",
      "\n",
      "PRACTICAL_NULL:\n",
      "  H₀: AI counting errors are within acceptable tolerance for research/clinical use (error ≤ 5%)\n",
      "\n",
      "PRACTICAL_ALTERNATIVE:\n",
      "  H₁: AI counting errors exceed acceptable tolerance for research/clinical use (error > 5%)\n",
      "\n",
      "📈 DESCRIPTIVE STATISTICS\n",
      "========================================\n",
      "True Count: 290\n",
      "AI Systems (n=3): [78, 91, 115]\n",
      "\n",
      "AI COUNTS:\n",
      "  Mean: 94.67\n",
      "  Std Dev: 18.77\n",
      "  Range: 78 - 115\n",
      "\n",
      "ABSOLUTE ERRORS:\n",
      "  Mean: 195.33\n",
      "  Std Dev: 18.77\n",
      "\n",
      "PERCENT ERRORS:\n",
      "  Mean: 67.4%\n",
      "  Std Dev: 6.5%\n",
      "\n",
      "🧪 HYPOTHESIS TESTS\n",
      "========================================\n",
      "\n",
      "ACCURACY_TEST:\n",
      "  Test: One-sample t-test\n",
      "  H₀: AI error mean = 0 (accurate)\n",
      "  p-value: 0.003064\n",
      "  Significant (α=0.05): True\n",
      "  Conclusion: Reject H₀ (AI is inaccurate)\n",
      "\n",
      "TOLERANCE_TEST:\n",
      "  Test: Practical tolerance assessment\n",
      "  H₀: AI errors ≤ 5% (acceptable)\n",
      "  Conclusion: 3/3 systems exceed 5% tolerance\n",
      "\n",
      "📏 EFFECT SIZES\n",
      "==============================\n",
      "cohens_d: -10.406 (Large effect)\n",
      "coefficient_of_variation: 19.828 (Moderate variability)\n",
      "mape: 67.356 (Poor accuracy)\n",
      "\n",
      "📋 EXCEL FORMULAS FOR VERIFICATION\n",
      "==================================================\n",
      "Copy these formulas into Excel to double-check calculations:\n",
      "\n",
      "standard_deviation: =STDEV.S(B2:B5)\n",
      "mean: =AVERAGE(B2:B5)\n",
      "median: =MEDIAN(B2:B5)\n",
      "variance: =VAR.S(B2:B5)\n",
      "range: =MAX(B2:B5)-MIN(B2:B5)\n",
      "mean_absolute_error: =AVERAGE(ABS(B2:B5-$C$1))\n",
      "mean_percent_error: =AVERAGE(ABS((B2:B5-$C$1)/$C$1)*100)\n",
      "coefficient_of_variation: =(STDEV.S(B2:B5)/AVERAGE(B2:B5))*100\n",
      "t_test_one_sample: =T.TEST(D2:D5,0,2,1)\n",
      "\n",
      "💾 Analysis exported to Excel: ai_counting_analysis.xlsx\n",
      "\n",
      "✅ ANALYSIS COMPLETE\n",
      "📋 Use the Excel file to verify calculations\n",
      "🧪 Statistical evidence for your research paper ready\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Statistical Analysis Helper for AI P0 Counting Research\n",
    "Null Hypothesis Testing & Standard Deviation Calculations\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_1samp, chi2_contingency, normaltest\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class StatisticalAnalysisHelper:\n",
    "    \"\"\"\n",
    "    Statistical analysis for AI counting accuracy research\n",
    "    Helps formulate null hypothesis and calculate key statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ai_counts = {}\n",
    "        self.true_count = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def set_data(self, ai_systems_counts: Dict[str, int], true_count: int):\n",
    "        \"\"\"Set the data for analysis\"\"\"\n",
    "        self.ai_counts = ai_systems_counts\n",
    "        self.true_count = true_count\n",
    "        \n",
    "        print(\"📊 DATA LOADED\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"True Count (Deterministic): {true_count}\")\n",
    "        for system, count in ai_systems_counts.items():\n",
    "            print(f\"{system}: {count}\")\n",
    "    \n",
    "    def formulate_null_hypothesis(self) -> Dict[str, str]:\n",
    "        \"\"\"Formulate appropriate null hypotheses for the research\"\"\"\n",
    "        \n",
    "        hypotheses = {\n",
    "            'accuracy_null': \"H₀: AI systems can accurately count P0 failures in documents (μ_error = 0)\",\n",
    "            'accuracy_alternative': \"H₁: AI systems cannot accurately count P0 failures in documents (μ_error ≠ 0)\",\n",
    "            \n",
    "            'consistency_null': \"H₀: AI systems produce consistent counts across different approaches (σ² = 0)\", \n",
    "            'consistency_alternative': \"H₁: AI systems produce inconsistent counts across different approaches (σ² > 0)\",\n",
    "            \n",
    "            'equivalence_null': \"H₀: AI counting performance is equivalent to deterministic methods (|μ_AI - μ_true| ≤ δ)\",\n",
    "            'equivalence_alternative': \"H₁: AI counting performance is not equivalent to deterministic methods (|μ_AI - μ_true| > δ)\",\n",
    "            \n",
    "            'practical_null': \"H₀: AI counting errors are within acceptable tolerance for research/clinical use (error ≤ 5%)\",\n",
    "            'practical_alternative': \"H₁: AI counting errors exceed acceptable tolerance for research/clinical use (error > 5%)\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\n🔬 NULL HYPOTHESES FORMULATED\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, hypothesis in hypotheses.items():\n",
    "            print(f\"\\n{key.upper()}:\")\n",
    "            print(f\"  {hypothesis}\")\n",
    "        \n",
    "        return hypotheses\n",
    "    \n",
    "    def calculate_descriptive_statistics(self) -> Dict:\n",
    "        \"\"\"Calculate key descriptive statistics\"\"\"\n",
    "        \n",
    "        ai_values = list(self.ai_counts.values())\n",
    "        errors = [abs(count - self.true_count) for count in ai_values]\n",
    "        percent_errors = [(abs(count - self.true_count) / self.true_count) * 100 for count in ai_values]\n",
    "        \n",
    "        stats_dict = {\n",
    "            'ai_counts': {\n",
    "                'mean': np.mean(ai_values),\n",
    "                'median': np.median(ai_values),\n",
    "                'std_dev': np.std(ai_values, ddof=1),  # Sample standard deviation\n",
    "                'variance': np.var(ai_values, ddof=1),\n",
    "                'min': np.min(ai_values),\n",
    "                'max': np.max(ai_values),\n",
    "                'range': np.max(ai_values) - np.min(ai_values)\n",
    "            },\n",
    "            'absolute_errors': {\n",
    "                'mean': np.mean(errors),\n",
    "                'median': np.median(errors),\n",
    "                'std_dev': np.std(errors, ddof=1),\n",
    "                'min': np.min(errors),\n",
    "                'max': np.max(errors)\n",
    "            },\n",
    "            'percent_errors': {\n",
    "                'mean': np.mean(percent_errors),\n",
    "                'median': np.median(percent_errors),\n",
    "                'std_dev': np.std(percent_errors, ddof=1),\n",
    "                'min': np.min(percent_errors),\n",
    "                'max': np.max(percent_errors)\n",
    "            },\n",
    "            'true_count': self.true_count,\n",
    "            'n_systems': len(ai_values)\n",
    "        }\n",
    "        \n",
    "        self.results['descriptive'] = stats_dict\n",
    "        \n",
    "        print(\"\\n📈 DESCRIPTIVE STATISTICS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"True Count: {self.true_count}\")\n",
    "        print(f\"AI Systems (n={len(ai_values)}): {ai_values}\")\n",
    "        print()\n",
    "        print(\"AI COUNTS:\")\n",
    "        print(f\"  Mean: {stats_dict['ai_counts']['mean']:.2f}\")\n",
    "        print(f\"  Std Dev: {stats_dict['ai_counts']['std_dev']:.2f}\")\n",
    "        print(f\"  Range: {stats_dict['ai_counts']['min']} - {stats_dict['ai_counts']['max']}\")\n",
    "        print()\n",
    "        print(\"ABSOLUTE ERRORS:\")\n",
    "        print(f\"  Mean: {stats_dict['absolute_errors']['mean']:.2f}\")\n",
    "        print(f\"  Std Dev: {stats_dict['absolute_errors']['std_dev']:.2f}\")\n",
    "        print()\n",
    "        print(\"PERCENT ERRORS:\")\n",
    "        print(f\"  Mean: {stats_dict['percent_errors']['mean']:.1f}%\")\n",
    "        print(f\"  Std Dev: {stats_dict['percent_errors']['std_dev']:.1f}%\")\n",
    "        \n",
    "        return stats_dict\n",
    "    \n",
    "    def perform_hypothesis_tests(self, alpha: float = 0.05) -> Dict:\n",
    "        \"\"\"Perform statistical hypothesis tests\"\"\"\n",
    "        \n",
    "        ai_values = list(self.ai_counts.values())\n",
    "        errors = [count - self.true_count for count in ai_values]  # Signed errors\n",
    "        abs_errors = [abs(error) for error in errors]\n",
    "        percent_errors = [(abs(count - self.true_count) / self.true_count) * 100 for count in ai_values]\n",
    "        \n",
    "        tests = {}\n",
    "        \n",
    "        # Test 1: One-sample t-test for accuracy (H₀: μ_error = 0)\n",
    "        if len(errors) > 1:\n",
    "            t_stat, p_value = ttest_1samp(errors, 0)\n",
    "            tests['accuracy_test'] = {\n",
    "                'test': 'One-sample t-test',\n",
    "                'null_hypothesis': 'AI error mean = 0 (accurate)',\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < alpha,\n",
    "                'conclusion': 'Reject H₀ (AI is inaccurate)' if p_value < alpha else 'Fail to reject H₀'\n",
    "            }\n",
    "        \n",
    "        # Test 2: Test if errors exceed practical tolerance (5%)\n",
    "        tolerance_violations = sum(1 for error in percent_errors if error > 5)\n",
    "        tests['tolerance_test'] = {\n",
    "            'test': 'Practical tolerance assessment',\n",
    "            'null_hypothesis': 'AI errors ≤ 5% (acceptable)',\n",
    "            'violations': tolerance_violations,\n",
    "            'total_systems': len(percent_errors),\n",
    "            'violation_rate': (tolerance_violations / len(percent_errors)) * 100,\n",
    "            'conclusion': f\"{tolerance_violations}/{len(percent_errors)} systems exceed 5% tolerance\"\n",
    "        }\n",
    "        \n",
    "        # Test 3: Normality test for errors\n",
    "        if len(errors) >= 8:  # Need sufficient sample size\n",
    "            normality_stat, normality_p = normaltest(errors)\n",
    "            tests['normality_test'] = {\n",
    "                'test': 'D\\'Agostino normality test',\n",
    "                'statistic': normality_stat,\n",
    "                'p_value': normality_p,\n",
    "                'normal': normality_p > alpha,\n",
    "                'conclusion': 'Errors are normally distributed' if normality_p > alpha else 'Errors are not normally distributed'\n",
    "            }\n",
    "        \n",
    "        self.results['hypothesis_tests'] = tests\n",
    "        \n",
    "        print(\"\\n🧪 HYPOTHESIS TESTS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for test_name, test_data in tests.items():\n",
    "            print(f\"\\n{test_name.upper()}:\")\n",
    "            print(f\"  Test: {test_data['test']}\")\n",
    "            print(f\"  H₀: {test_data['null_hypothesis']}\")\n",
    "            \n",
    "            if 'p_value' in test_data:\n",
    "                print(f\"  p-value: {test_data['p_value']:.6f}\")\n",
    "                print(f\"  Significant (α=0.05): {test_data.get('significant', 'N/A')}\")\n",
    "            \n",
    "            print(f\"  Conclusion: {test_data['conclusion']}\")\n",
    "        \n",
    "        return tests\n",
    "    \n",
    "    def calculate_effect_sizes(self) -> Dict:\n",
    "        \"\"\"Calculate effect sizes for practical significance\"\"\"\n",
    "        \n",
    "        ai_values = list(self.ai_counts.values())\n",
    "        errors = [count - self.true_count for count in ai_values]\n",
    "        \n",
    "        effect_sizes = {}\n",
    "        \n",
    "        # Cohen's d for error magnitude\n",
    "        if len(errors) > 1:\n",
    "            cohens_d = np.mean(errors) / np.std(errors, ddof=1)\n",
    "            effect_sizes['cohens_d'] = {\n",
    "                'value': cohens_d,\n",
    "                'interpretation': self._interpret_cohens_d(abs(cohens_d))\n",
    "            }\n",
    "        \n",
    "        # Coefficient of variation for consistency\n",
    "        if np.mean(ai_values) != 0:\n",
    "            cv = (np.std(ai_values, ddof=1) / np.mean(ai_values)) * 100\n",
    "            effect_sizes['coefficient_of_variation'] = {\n",
    "                'value': cv,\n",
    "                'interpretation': 'High variability' if cv > 20 else 'Moderate variability' if cv > 10 else 'Low variability'\n",
    "            }\n",
    "        \n",
    "        # Mean Absolute Percentage Error (MAPE)\n",
    "        mape = np.mean([abs(count - self.true_count) / self.true_count * 100 for count in ai_values])\n",
    "        effect_sizes['mape'] = {\n",
    "            'value': mape,\n",
    "            'interpretation': 'Poor accuracy' if mape > 20 else 'Fair accuracy' if mape > 10 else 'Good accuracy' if mape > 5 else 'Excellent accuracy'\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📏 EFFECT SIZES\")\n",
    "        print(\"=\" * 30)\n",
    "        for metric, data in effect_sizes.items():\n",
    "            print(f\"{metric}: {data['value']:.3f} ({data['interpretation']})\")\n",
    "        \n",
    "        return effect_sizes\n",
    "    \n",
    "    def _interpret_cohens_d(self, d: float) -> str:\n",
    "        \"\"\"Interpret Cohen's d effect size\"\"\"\n",
    "        if d < 0.2:\n",
    "            return \"Negligible effect\"\n",
    "        elif d < 0.5:\n",
    "            return \"Small effect\"\n",
    "        elif d < 0.8:\n",
    "            return \"Medium effect\"\n",
    "        else:\n",
    "            return \"Large effect\"\n",
    "    \n",
    "    def generate_excel_formulas(self) -> Dict[str, str]:\n",
    "        \"\"\"Generate Excel formulas for verification\"\"\"\n",
    "        \n",
    "        formulas = {\n",
    "            'standard_deviation': '=STDEV.S(B2:B5)',  # Assuming data in B2:B5\n",
    "            'mean': '=AVERAGE(B2:B5)',\n",
    "            'median': '=MEDIAN(B2:B5)',\n",
    "            'variance': '=VAR.S(B2:B5)',\n",
    "            'range': '=MAX(B2:B5)-MIN(B2:B5)',\n",
    "            'mean_absolute_error': '=AVERAGE(ABS(B2:B5-$C$1))',  # Assuming true value in C1\n",
    "            'mean_percent_error': '=AVERAGE(ABS((B2:B5-$C$1)/$C$1)*100)',\n",
    "            'coefficient_of_variation': '=(STDEV.S(B2:B5)/AVERAGE(B2:B5))*100',\n",
    "            't_test_one_sample': '=T.TEST(D2:D5,0,2,1)',  # Assuming errors in D2:D5\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📋 EXCEL FORMULAS FOR VERIFICATION\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Copy these formulas into Excel to double-check calculations:\")\n",
    "        print()\n",
    "        for name, formula in formulas.items():\n",
    "            print(f\"{name}: {formula}\")\n",
    "        \n",
    "        return formulas\n",
    "    \n",
    "    def create_summary_table(self) -> pd.DataFrame:\n",
    "        \"\"\"Create summary table for Excel export\"\"\"\n",
    "        \n",
    "        ai_values = list(self.ai_counts.values())\n",
    "        ai_names = list(self.ai_counts.keys())\n",
    "        \n",
    "        summary_data = []\n",
    "        for name, count in self.ai_counts.items():\n",
    "            error = count - self.true_count\n",
    "            abs_error = abs(error)\n",
    "            percent_error = (abs_error / self.true_count) * 100\n",
    "            \n",
    "            summary_data.append({\n",
    "                'AI_System': name,\n",
    "                'Count': count,\n",
    "                'True_Count': self.true_count,\n",
    "                'Error': error,\n",
    "                'Absolute_Error': abs_error,\n",
    "                'Percent_Error': percent_error,\n",
    "                'Within_5pct_Tolerance': 'Yes' if percent_error <= 5 else 'No'\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Add summary statistics row\n",
    "        summary_row = {\n",
    "            'AI_System': 'SUMMARY',\n",
    "            'Count': np.mean(ai_values),\n",
    "            'True_Count': self.true_count,\n",
    "            'Error': np.mean([row['Error'] for row in summary_data]),\n",
    "            'Absolute_Error': np.mean([row['Absolute_Error'] for row in summary_data]),\n",
    "            'Percent_Error': np.mean([row['Percent_Error'] for row in summary_data]),\n",
    "            'Within_5pct_Tolerance': f\"{sum(1 for row in summary_data if row['Within_5pct_Tolerance'] == 'Yes')}/{len(summary_data)}\"\n",
    "        }\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def export_to_excel(self, filename: str = 'ai_counting_analysis.xlsx'):\n",
    "        \"\"\"Export analysis to Excel for verification\"\"\"\n",
    "        \n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            # Summary table\n",
    "            summary_df = self.create_summary_table()\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            # Descriptive statistics\n",
    "            if 'descriptive' in self.results:\n",
    "                desc_data = []\n",
    "                for category, stats in self.results['descriptive'].items():\n",
    "                    if isinstance(stats, dict):\n",
    "                        for stat_name, value in stats.items():\n",
    "                            desc_data.append({\n",
    "                                'Category': category,\n",
    "                                'Statistic': stat_name,\n",
    "                                'Value': value\n",
    "                            })\n",
    "                \n",
    "                pd.DataFrame(desc_data).to_excel(writer, sheet_name='Descriptive_Stats', index=False)\n",
    "            \n",
    "            # Raw data for Excel calculations\n",
    "            raw_data = {\n",
    "                'AI_System': list(self.ai_counts.keys()),\n",
    "                'Count': list(self.ai_counts.values()),\n",
    "                'True_Count': [self.true_count] * len(self.ai_counts),\n",
    "                'Error': [count - self.true_count for count in self.ai_counts.values()],\n",
    "                'Absolute_Error': [abs(count - self.true_count) for count in self.ai_counts.values()],\n",
    "                'Percent_Error': [(abs(count - self.true_count) / self.true_count) * 100 for count in self.ai_counts.values()]\n",
    "            }\n",
    "            \n",
    "            pd.DataFrame(raw_data).to_excel(writer, sheet_name='Raw_Data', index=False)\n",
    "        \n",
    "        print(f\"\\n💾 Analysis exported to Excel: {filename}\")\n",
    "        return filename\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage with sample data\"\"\"\n",
    "    \n",
    "    print(\"📊 STATISTICAL ANALYSIS HELPER\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample data - replace with your actual values\n",
    "    ai_counts = {\n",
    "        'ChatGPT_Scan': 78,\n",
    "        'Claude_Addition': 91,\n",
    "        'Claude_Fabrication': 115,\n",
    "        #'Claude_Workup': 200\n",
    "    }\n",
    "    \n",
    "    true_count = 290  # Replace with your deterministic count\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = StatisticalAnalysisHelper()\n",
    "    analyzer.set_data(ai_counts, true_count)\n",
    "    \n",
    "    # Formulate hypotheses\n",
    "    hypotheses = analyzer.formulate_null_hypothesis()\n",
    "    \n",
    "    # Calculate descriptive statistics\n",
    "    desc_stats = analyzer.calculate_descriptive_statistics()\n",
    "    \n",
    "    # Perform hypothesis tests\n",
    "    test_results = analyzer.perform_hypothesis_tests()\n",
    "    \n",
    "    # Calculate effect sizes\n",
    "    effect_sizes = analyzer.calculate_effect_sizes()\n",
    "    \n",
    "    # Generate Excel formulas\n",
    "    excel_formulas = analyzer.generate_excel_formulas()\n",
    "    \n",
    "    # Export to Excel\n",
    "    excel_file = analyzer.export_to_excel()\n",
    "    \n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE\")\n",
    "    print(f\"📋 Use the Excel file to verify calculations\")\n",
    "    print(f\"🧪 Statistical evidence for your research paper ready\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa189dd-f22b-4ab4-a45b-d294c20beecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
