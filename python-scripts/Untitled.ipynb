{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff024e3-b935-4402-ae7c-6e69f846d9f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chatgpt_workup_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m EXPECTED_URLS = \u001b[32m18\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     n_urls = \u001b[38;5;28mlen\u001b[39m(\u001b[43mchatgpt_workup_urls\u001b[49m)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m n_urls == EXPECTED_URLS, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPECTED_URLS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ChatGPT workups, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_urls\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameError\u001b[39m: name 'chatgpt_workup_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # ðŸ”§ Consolidated Patch Cell â€” Combined Token Degradation Analysis\n",
    "# This cell fixes schema mismatches, enforces consistent indexing, strengthens modeling,\n",
    "# and removes random data fallbacks. Paste AFTER your data loading/extraction cells.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Sanity checks and warnings\n",
    "# -----------------------------\n",
    "# ChatGPT URLs: expect 18 (update if your list is intentionally 17)\n",
    "EXPECTED_URLS = 18\n",
    "try:\n",
    "    n_urls = len(chatgpt_workup_urls)\n",
    "    assert n_urls == EXPECTED_URLS, f\"Expected {EXPECTED_URLS} ChatGPT workups, found {n_urls}\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âš ï¸ URL count check: {e}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Normalize Claude schema: observed_tdt -> token_usage\n",
    "# ---------------------------------------------------\n",
    "print(f\"âœ… Claude session data loaded (n={len(claude_sessions)})\")\n",
    "\n",
    "claude_df = pd.DataFrame(claude_sessions).copy()\n",
    "# Align to shared schema fields used later\n",
    "claude_df['token_usage'] = pd.to_numeric(claude_df['observed_tdt'], errors='coerce')\n",
    "claude_df['threshold']   = claude_df['token_usage']  # mirror for downstream code\n",
    "\n",
    "# Ensure minimal required columns exist\n",
    "for col in ['session', 'date', 'ai_system', 'token_usage']:\n",
    "    if col not in claude_df.columns:\n",
    "        claude_df[col] = np.nan\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2) Ensure ChatGPT extracted frame is present\n",
    "#    (No random data fallback: keep only real)\n",
    "# -------------------------------------------\n",
    "if 'chatgpt_sessions' not in globals():\n",
    "    print(\"âš ï¸ chatgpt_sessions not found; creating empty DataFrame\")\n",
    "    chatgpt_df = pd.DataFrame(columns=['session','date','ai_system','token_usage','threshold','evidence'])\n",
    "else:\n",
    "    chatgpt_df = pd.DataFrame(chatgpt_sessions).copy()\n",
    "\n",
    "# Coerce numeric and clean\n",
    "for df_ in [claude_df, chatgpt_df]:\n",
    "    for col in ['session', 'token_usage']:\n",
    "        if col in df_.columns:\n",
    "            df_[col] = pd.to_numeric(df_[col], errors='coerce')\n",
    "\n",
    "# ------------------------------------\n",
    "# 3) Build combined_df with global index\n",
    "# ------------------------------------\n",
    "def _ts_key(df):\n",
    "    # robust datetime parse\n",
    "    return pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "claude_df['_order']  = _ts_key(claude_df)\n",
    "chatgpt_df['_order'] = _ts_key(chatgpt_df)\n",
    "\n",
    "combined_df = pd.concat([claude_df, chatgpt_df], ignore_index=True)\n",
    "\n",
    "# Drop rows without token_usage\n",
    "combined_df = combined_df.dropna(subset=['token_usage']).copy()\n",
    "\n",
    "# Sort chronologically, then by system and session to be stable\n",
    "combined_df = combined_df.sort_values(['_order', 'ai_system', 'session'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "# Create a unified global session index for modeling\n",
    "combined_df['global_session'] = np.arange(1, len(combined_df) + 1)\n",
    "\n",
    "# Dynamic counts\n",
    "n_total   = len(combined_df)\n",
    "n_claude  = int((combined_df['ai_system'] == 'Claude').sum())\n",
    "n_chatgpt = int((combined_df['ai_system'] == 'ChatGPT').sum())\n",
    "\n",
    "print(\"\\nðŸ”— COMBINED DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total sessions: {n_total}\")\n",
    "print(f\"Claude sessions: {n_claude}\")\n",
    "print(f\"ChatGPT sessions: {n_chatgpt}\")\n",
    "print(f\"Sample size (n): {n_total}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4) Summary stats (clean NaNs beforehand)\n",
    "# ----------------------------------------\n",
    "all_token_usage = combined_df['token_usage'].dropna().to_numpy()\n",
    "claude_tokens   = combined_df.loc[combined_df['ai_system']=='Claude','token_usage'].dropna().to_numpy()\n",
    "chatgpt_tokens  = combined_df.loc[combined_df['ai_system']=='ChatGPT','token_usage'].dropna().to_numpy()\n",
    "\n",
    "print(f\"\\nðŸ“Š SUMMARY BY AI SYSTEM:\")\n",
    "summary_stats = (combined_df\n",
    "                 .groupby('ai_system')['token_usage']\n",
    "                 .agg(['count','mean','std','min','max'])\n",
    "                 .round(2))\n",
    "display(summary_stats)\n",
    "\n",
    "print(f\"\\nðŸ“‹ COMBINED DATASET (head):\")\n",
    "display(combined_df.head())\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Statistical tests (Chi-square, t-test)\n",
    "# -----------------------------------------\n",
    "print(\"\\nðŸ“Š STATISTICAL ANALYSIS - COMBINED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "n = len(all_token_usage)\n",
    "if n >= 2:\n",
    "    print(f\"ðŸŽ¯ OVERALL STATISTICS (n={n}):\")\n",
    "    print(f\"  Mean token usage: {np.mean(all_token_usage):.2f}%\")\n",
    "    print(f\"  Std deviation:    {np.std(all_token_usage, ddof=1):.2f}\")\n",
    "    print(f\"  Variance:         {np.var(all_token_usage, ddof=1):.2f}\")\n",
    "    print(f\"  Range:            {np.min(all_token_usage)}% - {np.max(all_token_usage)}%\")\n",
    "\n",
    "    # Chi-square variance test: H0: ÏƒÂ² = 10\n",
    "    null_variance = 10\n",
    "    sample_variance = float(np.var(all_token_usage, ddof=1))\n",
    "    chi2_stat = (n - 1) * sample_variance / null_variance\n",
    "    p_value = 1 - stats.chi2.cdf(chi2_stat, n - 1)\n",
    "\n",
    "    print(f\"\\nðŸ§ª CHI-SQUARE VARIANCE TEST:\")\n",
    "    print(f\"  Hâ‚€: ÏƒÂ² = {null_variance}\")\n",
    "    print(f\"  Sample variance:      {sample_variance:.3f}\")\n",
    "    print(f\"  Chi-square statistic: {chi2_stat:.3f}\")\n",
    "    print(f\"  Degrees of freedom:   {n-1}\")\n",
    "    print(f\"  p-value:              {p_value:.6f}\")\n",
    "    print(f\"  Reject Hâ‚€:            {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Not enough observations for variance test.\")\n",
    "\n",
    "# Two-sample t-test if both arms have at least 2\n",
    "if len(claude_tokens) > 1 and len(chatgpt_tokens) > 1:\n",
    "    t_stat, t_p_value = stats.ttest_ind(claude_tokens, chatgpt_tokens, equal_var=False)\n",
    "    print(f\"\\nðŸ¤– CLAUDE vs CHATGPT (two-sample t-test, Welch):\")\n",
    "    print(f\"  Claude mean:   {np.mean(claude_tokens):.2f}% (n={len(claude_tokens)})\")\n",
    "    print(f\"  ChatGPT mean:  {np.mean(chatgpt_tokens):.2f}% (n={len(chatgpt_tokens)})\")\n",
    "    print(f\"  t-statistic:   {t_stat:.3f}\")\n",
    "    print(f\"  p-value:       {t_p_value:.6f}\")\n",
    "    print(f\"  Significant:   {'Yes' if t_p_value < 0.05 else 'No'}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Not enough data in both groups for t-test.\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) Degradation modeling using global_session idx\n",
    "# ------------------------------------------------\n",
    "def analyze_token_degradation(df):\n",
    "    \"\"\"Analyze token degradation patterns using global_session.\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    df_sorted = df.sort_values('global_session')\n",
    "    x = df_sorted['global_session'].to_numpy()\n",
    "    y = df_sorted['token_usage'].to_numpy()\n",
    "\n",
    "    print(\"\\nðŸ“‰ TOKEN DEGRADATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # Linear\n",
    "    lin = LinearRegression().fit(x.reshape(-1,1), y)\n",
    "    yhat = lin.predict(x.reshape(-1,1))\n",
    "    r2 = r2_score(y, yhat)\n",
    "\n",
    "    models['linear'] = {\n",
    "        'slope': lin.coef_[0],\n",
    "        'intercept': lin.intercept_,\n",
    "        'r_squared': r2,\n",
    "        'model': lin\n",
    "    }\n",
    "    print(f\"LINEAR MODEL:\")\n",
    "    print(f\"  y = {lin.coef_[0]:.3f} * global_session + {lin.intercept_:.3f}\")\n",
    "    print(f\"  RÂ² = {r2:.3f}\")\n",
    "    print(f\"  Slope: {lin.coef_[0]:+.3f}% per global session\")\n",
    "\n",
    "    # Exponential (robust init)\n",
    "    def fexp(x, A, k, C): return A * np.exp(-k * x) + C\n",
    "    try:\n",
    "        p0 = [float(np.ptp(y)) if len(y) else 1.0, 0.01, float(np.min(y)) if len(y) else 0.0]\n",
    "        popt, _ = curve_fit(fexp, x, y, p0=p0, maxfev=10000)\n",
    "        yexp = fexp(x, *popt)\n",
    "        r2e = r2_score(y, yexp)\n",
    "        models['exponential'] = {\n",
    "            'A': popt[0], 'k': popt[1], 'C': popt[2],\n",
    "            'r_squared': r2e,\n",
    "            'half_life': (np.log(2)/popt[1]) if popt[1] > 0 else np.inf\n",
    "        }\n",
    "        print(f\"\\nEXPONENTIAL MODEL:\")\n",
    "        print(f\"  RÂ² = {r2e:.3f}\")\n",
    "        print(f\"  Half-life: {models['exponential']['half_life']:.2f} global sessions\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nEXPONENTIAL MODEL: Failed to fit - {e}\")\n",
    "        models['exponential'] = None\n",
    "\n",
    "    return models, df_sorted\n",
    "\n",
    "degradation_models, df_temporal = analyze_token_degradation(combined_df)\n",
    "\n",
    "# ----------------\n",
    "# 7) Visualization\n",
    "# ----------------\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Split groups\n",
    "claude_data = combined_df[combined_df['ai_system'] == 'Claude']\n",
    "chatgpt_data = combined_df[combined_df['ai_system'] == 'ChatGPT']\n",
    "\n",
    "# Plot 1: token usage over time (global_session)\n",
    "ax1.scatter(claude_data['global_session'], claude_data['token_usage'], \n",
    "            color='blue', s=80, alpha=0.75, label='Claude', marker='o')\n",
    "ax1.scatter(chatgpt_data['global_session'], chatgpt_data['token_usage'], \n",
    "            color='red', s=80, alpha=0.75, label='ChatGPT', marker='s')\n",
    "\n",
    "# Linear trend\n",
    "if degradation_models.get('linear'):\n",
    "    x_trend = np.linspace(combined_df['global_session'].min(), combined_df['global_session'].max(), 200)\n",
    "    y_trend = degradation_models['linear']['model'].predict(x_trend.reshape(-1, 1))\n",
    "    ax1.plot(x_trend, y_trend, '--', color='gray', alpha=0.8, label='Linear Trend')\n",
    "\n",
    "ax1.axhline(y=85, color='orange', linestyle=':', alpha=0.8, label='Critical Threshold (85%)')\n",
    "ax1.set_xlabel('Global Session')\n",
    "ax1.set_ylabel('Token Usage (%)')\n",
    "ax1.set_title('Token Usage Over Time (Global Index)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution comparison (histograms)\n",
    "ax2.hist(claude_tokens, bins=10, alpha=0.6, label='Claude', color='blue', density=True)\n",
    "ax2.hist(chatgpt_tokens, bins=10, alpha=0.6, label='ChatGPT', color='red', density=True)\n",
    "ax2.set_xlabel('Token Usage (%)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Token Usage Distribution by AI System')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Boxplot comparison\n",
    "ax3.boxplot([claude_tokens, chatgpt_tokens], labels=['Claude', 'ChatGPT'])\n",
    "ax3.set_ylabel('Token Usage (%)')\n",
    "ax3.set_title('Token Usage Distribution Comparison')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Frequency distribution of token_usage values\n",
    "vals, counts = np.unique(combined_df['token_usage'].round(), return_counts=True)\n",
    "ax4.bar(vals, counts, alpha=0.75, color='green')\n",
    "ax4.axvline(x=85, color='red', linestyle='--', label='Critical Threshold')\n",
    "ax4.set_xlabel('Token Usage (%) (rounded)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Token Usage Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8) 100-session linear prediction\n",
    "# -------------------------------\n",
    "def predict_100_sessions_combined(models):\n",
    "    print(\"\\nðŸ”® 100-SESSION PREDICTION\")\n",
    "    print(\"=\" * 40)\n",
    "    future_sessions = np.arange(1, 101)  # future global sessions\n",
    "\n",
    "    if models.get('linear'):\n",
    "        linear_pred = models['linear']['model'].predict(future_sessions.reshape(-1, 1))\n",
    "        print(\"Session | Token Usage\")\n",
    "        print(\"--------|------------\")\n",
    "        for s in [30, 50, 75, 100]:\n",
    "            print(f\"{s:7d} | {linear_pred[s-1]:6.1f}%\")\n",
    "        crit = np.where(linear_pred >= 85)[0]\n",
    "        if len(crit):\n",
    "            print(f\"\\nLast global session â‰¥85%: {crit[-1] + 1}\")\n",
    "        return linear_pred\n",
    "    return None\n",
    "\n",
    "_ = predict_100_sessions_combined(degradation_models)\n",
    "\n",
    "# ----------------\n",
    "# 9) Export results\n",
    "# ----------------\n",
    "combined_df.to_csv('combined_token_analysis.csv', index=False)\n",
    "\n",
    "summary_report = {\n",
    "    'total_sessions': int(len(combined_df)),\n",
    "    'claude_sessions': int(n_claude),\n",
    "    'chatgpt_sessions': int(n_chatgpt),\n",
    "    'overall_mean_token_usage': float(np.mean(all_token_usage)) if len(all_token_usage) else None,\n",
    "    'overall_std_token_usage': float(np.std(all_token_usage, ddof=1)) if len(all_token_usage)>1 else None,\n",
    "    'overall_variance': float(np.var(all_token_usage, ddof=1)) if len(all_token_usage)>1 else None,\n",
    "}\n",
    "\n",
    "# Include chi-square only if computed above\n",
    "if n >= 2:\n",
    "    summary_report['chi_square_test'] = {\n",
    "        'null_hypothesis': 'ÏƒÂ² = 10',\n",
    "        'test_statistic': float(chi2_stat),\n",
    "        'p_value': float(p_value),\n",
    "        'reject_null': bool(p_value < 0.05)\n",
    "    }\n",
    "\n",
    "# Include t-test only if computed\n",
    "if len(claude_tokens) > 1 and len(chatgpt_tokens) > 1:\n",
    "    summary_report['claude_vs_chatgpt'] = {\n",
    "        'claude_mean': float(np.mean(claude_tokens)),\n",
    "        'chatgpt_mean': float(np.mean(chatgpt_tokens)),\n",
    "        't_test_p_value': float(t_p_value)\n",
    "    }\n",
    "\n",
    "import json\n",
    "with open('token_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"\\nðŸ’¾ EXPORT COMPLETE\")\n",
    "print(\"=\" * 30)\n",
    "print(\"Files created:\")\n",
    "print(\"â€¢ combined_token_analysis.csv\")\n",
    "print(\"â€¢ token_analysis_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be977f8-9200-4e8b-bfa3-7d19703f9f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
